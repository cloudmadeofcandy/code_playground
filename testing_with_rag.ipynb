{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN THIS NOTEBOOK, WE SHALL USE LANGCHAIN'S CHROMADB INTERFACE TO TURN THE (REALLY COOL LLM) INTO ONE W/ RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # Importing text splitter from Langchain\n",
    "from langchain.schema import Document # Importing Document schema from Langchain\n",
    "from langchain.vectorstores.chroma import Chroma # Importing Chroma vector store from Langchain\n",
    "from langchain.\n",
    "import os # Importing os module for operating system functionalities\n",
    "import shutil # Importing shutil module for high-level file operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Acknowledgments\n",
      "\n",
      "Books like this involve work from many people, and as authors, we truly appreciate the hard work and dedication that the team at Wiley shows. We would especially like to thank Senior Acquisitions Editor Kenyon Brown. We have collaborated with Ken on multiple projects and consistently enjoy our work with him.\n",
      "\n",
      "We owe a great debt of gratitude to Runzhi “Tom” Song, Mike's research assistant at Notre Dame. Tom's assistance with the instructional materials that accompany this book was invaluable.\n",
      "\n",
      "We also greatly appreciate the editing and production team for this book, including Lily Miller, our project editor, who brought years of experience and great talent to the project; Chris Crayton, our technical editor, and Shahla Pirnia, our technical proofreader who both provided insightful advice and gave wonderful feedback throughout the book; and Saravanan Dakshinamurthy, our production editor, who guided us through layouts, formatting, and final cleanup to produce a great book. We would also like to thank the many behind-the-scenes contributors, including the graphics, production, and technical teams who make the book and companion materials into a finished product.\n",
      "\n",
      "Our agent, Carole Jelen of Waterside Productions, continues to provide us with wonderful opportunities, advice, and assistance throughout our writing careers.\n",
      "\n",
      "Finally, we would like to thank our families and significant others who support us through the late evenings, busy weekends, and long hours that a book like this requires to write, edit, and get to press.' metadata={'source': 'data/processed/comptia_security_plus.epub/OPS/f05.xhtml'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import UnstructuredXMLLoader\n",
    "\n",
    "file_path = r'data/processed/comptia_security_plus.epub/OPS'\n",
    "\n",
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def load_xhtml_file(file_path):\n",
    "    list_of_documents = []\n",
    "    list_of_files = os.listdir(file_path)\n",
    "    for file in list_of_files:\n",
    "        if file.endswith('.xhtml'):\n",
    "            documents = UnstructuredXMLLoader(file_path + '/' + file).load()\n",
    "            list_of_documents.append(documents)\n",
    "    list_of_documents = flatten(list_of_documents)\n",
    "    return list_of_documents \n",
    "\n",
    "documents = load_xhtml_file(file_path=file_path) # Call the function\n",
    "# Inspect the contents of the first document as well as metadata\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "  \"\"\"\n",
    "  Split the text content of the given list of Document objects into smaller chunks.\n",
    "  Args:\n",
    "    documents (list[Document]): List of Document objects containing text content to split.\n",
    "  Returns:\n",
    "    list[Document]: List of Document objects representing the split text chunks.\n",
    "  \"\"\"\n",
    "  # Initialize text splitter with specified parameters\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, # Size of each chunk in characters\n",
    "    chunk_overlap=100, # Overlap between consecutive chunks\n",
    "    length_function=len, # Function to compute the length of the text\n",
    "    add_start_index=True, # Flag to add start index to each chunk\n",
    "  )\n",
    "\n",
    "  # Split documents into smaller chunks using text splitter\n",
    "  chunks = text_splitter.split_documents(documents)\n",
    "  print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "  # Print example of page content and metadata for a chunk\n",
    "  document = chunks[0]\n",
    "  print(document.page_content)\n",
    "  print(document.metadata)\n",
    "\n",
    "  return chunks # Return the list of split text chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "  \"\"\"\n",
    "  Save the given list of Document objects to a Chroma database.\n",
    "  Args:\n",
    "  chunks (list[Document]): List of Document objects representing text chunks to save.\n",
    "  Returns:\n",
    "  None\n",
    "  \"\"\"\n",
    "\n",
    "  # Clear out the existing database directory if it exists\n",
    "  if os.path.exists(CHROMA_PATH):\n",
    "    shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "  # Create a new Chroma database from the documents using OpenAI embeddings\n",
    "  db = Chroma.from_documents(\n",
    "    chunks,\n",
    "    ,\n",
    "    persist_directory=CHROMA_PATH\n",
    "  )\n",
    "\n",
    "  # Persist the database to disk\n",
    "  db.persist()\n",
    "  print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
