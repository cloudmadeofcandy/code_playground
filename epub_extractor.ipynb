{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import stuff :)\n",
    "\n",
    "from os import path, listdir\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "import lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/epub/comptia_security_plus.epub'\n",
    "extract_dir = 'data/processed'\n",
    "\n",
    "with ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir + '/' + filename.split('data/epub/')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html epub:prefix=\"index: http://www.index.com/\" lang=\"en\" xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:epub=\"http://www.idpf.org/2007/ops\" xmlns:svg=\"http://www.w3.org/2000/svg\">\n",
      "<head>\n",
      "<title>Acknowledgments</title>\n",
      "<link href=\"WileyTemplate_v5.5.css\" rel=\"stylesheet\" type=\"text/css\"/>\n",
      "<meta content=\"urn:uuid:57d469d3-a1d3-48bd-8447-18ce82f57cc2\" name=\"Adept.expected.resource\"/>\n",
      "</head>\n",
      "<body epub:type=\"frontmatter\">\n",
      "<section aria-labelledby=\"f06_1\" epub:type=\"acknowledgments\" role=\"doc-acknowledgments\">\n",
      "<header>\n",
      "<h1 id=\"f06_1\"><span aria-label=\"ix\" epub:type=\"pagebreak\" id=\"Page_ix\" role=\"doc-pagebreak\"></span><span id=\"fm5\"></span>Acknowledgments</h1>\n",
      "</header>\n",
      "<section aria-label=\"chapter opening\">\n",
      "<p id=\"ffirs-para-0031\">Books like this involve work from many people, and as authors, we truly appreciate the hard work and dedication that the team at Wiley shows. We would especially like to thank Senior Acquisitions Editor Kenyon Brown. We have collaborated with Ken on multiple projects and consistently enjoy our work with him.</p>\n",
      "<p id=\"ffirs-para-0032\">We owe a great debt of gratitude to Runzhi “Tom” Song, Mike's research assistant at Notre Dame. Tom's assistance with the instructional materials that accompany this book was invaluable.</p>\n",
      "<p id=\"ffirs-para-0033\">We also greatly appreciate the editing and production team for this book, including Lily Miller, our project editor, who brought years of experience and great talent to the project; Chris Crayton, our technical editor, and Shahla Pirnia, our technical proofreader who both provided insightful advice and gave wonderful feedback throughout the book; and Saravanan Dakshinamurthy, our production editor, who guided us through layouts, formatting, and final cleanup to produce a great book. We would also like to thank the many behind-the-scenes contributors, including the graphics, production, and technical teams who make the book and companion materials into a finished product.</p>\n",
      "<p id=\"ffirs-para-0034\">Our agent, Carole Jelen of Waterside Productions, continues to provide us with wonderful opportunities, advice, and assistance throughout our writing careers.</p>\n",
      "<p id=\"ffirs-para-0035\">Finally, we would like to thank our families and significant others who support us through the late evenings, busy weekends, and long hours that a book like this requires to write, edit, and get to press.</p>\n",
      "</section>\n",
      "</section>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "for files in listdir(extract_dir + '/' + filename.split('data/epub/')[-1] + '/OPS'):\n",
    "    if (files.endswith('.xhtml')):\n",
    "        with open(extract_dir + '/' + filename.split('data/epub/')[-1] + '/OPS/' + files, 'r') as file:\n",
    "            soup = BeautifulSoup(file, 'lxml')\n",
    "            print(soup)\n",
    "            break\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D\n"
     ]
    }
   ],
   "source": [
    "# for jesting purposes\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "question_keys = {}\n",
    "with open('data/processed/comptia_security_plus.epub/OPS/b01.xhtml', 'r') as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "    result_set = soup.find_all('li', id=re.compile('bapp01-ex-'))\n",
    "    for result in result_set:\n",
    "        print(result.get_text().split('. ')[0])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for jesting purposes\n",
    "from bs4 import BeautifulSoup\n",
    "import re, json\n",
    "question_keys = {}\n",
    "with open('data/processed/comptia_security_plus.epub/OPS/b01.xhtml', 'r') as file:\n",
    "    soup = BeautifulSoup(file, 'html.parser')\n",
    "    result_set = soup.find_all('li', id=re.compile('bapp01-ex-'))\n",
    "    for result in result_set:\n",
    "        a = result.get_text().partition('. ')\n",
    "        question_keys[int(result['id'].split('bapp01-ex-')[-1])] = {'answer_key':a[0], 'explaination':a[-1]}\n",
    "    with open('data/processed/comptia_security_plus_output/answer_key.json', 'w') as convert_file: \n",
    "        convert_file.write(json.dumps(question_keys, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re, json\n",
    "\n",
    "question_list = {}\n",
    "question_index = 1\n",
    "for i in range(1, 18):\n",
    "    chapter = str(i).zfill(2)\n",
    "    with open(f'data/processed/comptia_security_plus.epub/OPS/c{chapter}.xhtml', 'r') as file:\n",
    "        soup = BeautifulSoup(file, 'lxml')\n",
    "        result_set = soup.find_all('li', id=re.compile(f'c{chapter}-ex-'))\n",
    "        for result in result_set:\n",
    "            question_set = {}\n",
    "            answer_ol = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "            question_set['question'] = result.get_text().split('\\n\\n')[0]\n",
    "            key = result.find_all('li')\n",
    "            question_set['answer'] = {answer_ol[i]:key[i].get_text() for i in range(len(key))}\n",
    "            question_list.update({question_index: question_set})\n",
    "            question_index += 1\n",
    "        with open('data/processed/comptia_security_plus_output/questions.json', 'w') as convert_file: \n",
    "            convert_file.write(json.dumps(question_list, indent=4))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
